#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed Feb  9 20:29:22 2022

@author: pooyan
"""

import pandas as pd
import numpy as np 
#### 
from sklearn.model_selection import train_test_split , StratifiedKFold, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from xgboost import XGBClassifier
#df = pd.read_csv('./2K_1420_part_1234567features.csv',delimiter=';',index_col=False) 




df = pd.read_csv('/Users/pooyan/Documents/Emerson/data/data.csv', index_col=False, sep=";", header=0)
#set 0 as normal as 1 as abnormal
df.insert(3, 'target', 0)

# df.drop('Rows Header(1)')
df



indexes = []
df['target']= 0

indexes = df[df['RU.LNG.214LRSA450.PV'] <= 47].index | \
df[df['RU.LNG.214LRSA450.PV'] <= 45].index | \
df[df['RU.LNG.214TRA220.PV'] > df['RU.LNG.214TRA220.PV'].quantile(0.99)].index  |\
df[df['RU.LNG.214PRA218C.PV'] < df['RU.LNG.214PRA218C.PV'].quantile(0.01)].index |\
df[df['RU.LNG.214TRA476A.PV'] < df['RU.LNG.214TRA476A.PV'].quantile(0.01)].index  |\
df[df['RU.LNG.214PZA451A.PV'] < df['RU.LNG.214PZA451A.PV'].quantile(0.01)].index |\
df[df['RU.LNG.214PDRA425.PV'] < df['RU.LNG.214PDRA425.PV'].quantile(0.01)].index |\
df[df['RU.LNG.214PRA431.PV'] > df['RU.LNG.214PRA431.PV'].quantile(0.99)].index  |\
df[df['RU.LNG.214PRA434.PV'] > df['RU.LNG.214PRA434.PV'].quantile(0.99)].index  |\
df[df['RU.LNG.214PRA218.PV'] < df['RU.LNG.214PRA218.PV'].quantile(0.01)].index  |\
df[df['RU.LNG.214PDRA436.PV'] < df['RU.LNG.214PDRA436.PV'].quantile(0.01)].index |\
df[df['RU.LNG.214SZA458.PV'] < df['RU.LNG.214SZA458.PV'].quantile(0.01)].index |\
df[df['RU.LNG.214PRA221A.PV'] < df['RU.LNG.214PRA221A.PV'].quantile(0.01)].index |\
df[df['RU.LNG.214PRA221B.PV'] < df['RU.LNG.214PRA221B.PV'].quantile(0.01)].index |\
df[df['RU.LNG.214TR210.PV'] > df['RU.LNG.214TR210.PV'].quantile(0.99)].index |\
df[df['RU.LNG.214PRA218A.PV'] < df['RU.LNG.214PRA218A.PV'].quantile(0.01)].index 





df.loc[indexes, 'target'] = 1
print("Abnormal samples: ",len(df[df['target']==1]), "Normal samples: ",len(df[df['target']==0]) )


X = df.loc[:,'RU.LNG.214PZA451A.PV':].to_numpy() # all measurement columns
y = df['target'].to_numpy() # labels 









# split into train test sets
#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)
# scale data
#t = MinMaxScaler()
#t.fit(X_train)
#X_train = t.transform(X_train)
#X_test = t.transform(X_test)

train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=42)

# reshape input to be 3D [samples, timesteps, features]
train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))
test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))
print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)


#build a model
from keras.models import Sequential
from keras.layers import LSTM, Dropout, Dense
from keras.losses import SparseCategoricalCrossentropy


# design network
model = Sequential()
model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))
model.add(Dense(1))
model.compile(loss='mae', optimizer='adam', metrics=["accuracy"])

#src: https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/

model.summary()


history = model.fit(train_X, train_y, epochs=50, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=True)




#predict

yhat = model.predict(test_X)
# plot history


scores = model.evaluate(test_X, yhat)

LSTM_accuracy = scores[1]*100

print('Test accuracy: ', scores[1]*100, '%')



# summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'][1:43])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()
 
plt.hist(history.history['val_accuracy'])
plt.title('model loss')
plt.hist(history.history['loss'])
plt.title('model loss')
#plt.ylabel('loss')
#plt.xlabel('epoch')
import matplotlib.pyplot as plt
import pyplot
# plot history
pyplot.plot(history.history['loss'], label='train')
pyplot.plot(history.history['val_loss'], label='test')
pyplot.legend()
pyplot.show()








##test another network


train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=42)


# reshape input to be 3D [samples, timesteps, features]
train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))
test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))
print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)


#split data into train. validation, and test
X_train, X_test, y_train, y_test = train_test_split(all_data,all_label,test_size=0.1,shuffle=True)
X_train, X_validation, y_train, y_validation = train_test_split(X_train,y_train,test_size=0.1,shuffle=True)





model = Sequential()
model.add(LSTM(128, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))
model.add(LSTM(64, activation='relu', return_sequences=False))
model.add(RepeatVector(X_train.shape[1]))
model.add(LSTM(64, activation='relu', return_sequences=True))
model.add(LSTM(128, activation='relu', return_sequences=True))
model.add(TimeDistributed(Dense(X_train.shape[2])))
model.add(LSTM(64, activation='relu', return_sequences=False))
model.add(Dense(num_classes, activation = "softmax"))
model.compile(optimizer='adam', loss='mse', metrics=["accuracy"])
model.summary()




num_classes=2
from keras.layers import RepeatVector
from keras.layers import TimeDistributed

n_unit=50

model = Sequential()
model.add(LSTM(n_unit, input_shape=(X_train.shape[1], X_train.shape[2])))
model.add(Dropout(rate=0.2))
model.add(RepeatVector(dim3)) 
model.add(LSTM(n_unit, return_sequences=True))
model.add(Dense(num_classes, activation = "linear"))
model.add(Dropout(rate=0.2))
model.add(TimeDistributed(Dense(n_features))) #applies a specific layer such as Dense to every sample it receives as an input. 
model.add(LSTM(n_unit, input_shape=(n_features,dim3)))
model.add(Dense(num_classes, activation = "softmax"))
model.compile(optimizer=keras.optimizers.Adam(1e-4),loss=SparseCategoricalCrossentropy(), metrics=["accuracy"])
model.summary()

###gru




#gru

num_classes=2

import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

from keras.layers import GRU

model = Sequential()
model.add(GRU(10, input_shape=(X_train.shape[1], X_train.shape[2])))
model.add(Dropout(0.1)) #recommnedded rate is 0.1
#model.add(Dense(1, activation="sigmoid"))
model.add(Dense(num_classes, activation = "softmax"))
#model.add(Dense(num_classes, activation = "sigmoid"))
model.compile(optimizer=keras.optimizers.Adam(1e-4),loss= SparseCategoricalCrossentropy(), metrics=["accuracy"])

#model.compile(optimizer=keras.optimizers.Adam(1e-4),loss='mse', metrics=["accuracy"])


model.summary()

####



model = Sequential()
model.add(CuDNNLSTM(128, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))
model.add(Dropout(0.2))
model.add(BatchNormalization())  #normalizes activation outputs, same reason you want to normalize your input data.

model.add(CuDNNLSTM(128, return_sequences=True))
model.add(Dropout(0.1))
model.add(BatchNormalization())

model.add(CuDNNLSTM(128))
model.add(Dropout(0.2))
model.add(BatchNormalization())

model.add(Dense(32, activation='relu'))
model.add(Dropout(0.2))

model.add(Dense(2, activation='softmax'))






###


# design network
model = Sequential()
model.add(GRU(50, input_shape=(train_X.shape[1], train_X.shape[2])))
model.add(Dense(1, activation="softmax"))
model.compile(loss='mae', optimizer='adam', metrics=["accuracy"])

#src: https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/

model.summary()





history = model.fit(train_X, train_y, epochs=50, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=True)



yhat = model.predict(test_X)




##

# design network dense
model = Sequential()
model.add(Dense(50, input_shape=(train_X.shape[1], train_X.shape[2])))
model.add(Dense(1, activation="softmax"))
model.compile(loss='mse', optimizer='adam', metrics=["accuracy"])

#src: https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/

model.summary()


##
model = Sequential()  #define model
model.add(Dense(12, input_shape=(train_X.shape[1], train_X.shape[2]), activation="relu"))
model.add(Dense(8, activation="relu"))
model.add(Dense(1, activation="sigmoid"))
model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"]) #compile model
model.fit(x,y, epochs=150, batch_size=10)  #training




history = model.fit(train_X, train_y, epochs=50, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=True)



scores = model.evaluate(test_X, yhat)

Dense_accuracy = scores[1]*100

print('Test accuracy: ', scores[1]*100, '%')